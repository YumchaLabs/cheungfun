//! Query Pipeline Demo
//!
//! Demonstrates the query processing pipeline in cheungfun-query:
//! - Memory management for conversation context
//! - Response generation with different strategies
//! - Query optimization and caching

use cheungfun_core::{
    traits::{BaseMemory, Embedder, ResponseGenerator},
    types::{GenerationOptions, Query, SearchMode},
    ChatMessage, MessageRole, Node, ScoredNode,
};
use cheungfun_query::memory::{ChatMemoryBuffer, ChatMemoryConfig};
use std::{collections::HashMap, sync::Arc};
use uuid::Uuid;

// Mock embedder for demonstration
#[derive(Debug)]
struct MockEmbedder;

#[async_trait::async_trait]
impl Embedder for MockEmbedder {
    async fn embed(&self, text: &str) -> cheungfun_core::Result<Vec<f32>> {
        // Simple hash-based mock embedding
        let hash = text.len() as f32;
        Ok(vec![
            hash / 1000.0,
            (hash * 2.0) / 1000.0,
            (hash * 3.0) / 1000.0,
        ])
    }

    async fn embed_batch(&self, texts: Vec<&str>) -> cheungfun_core::Result<Vec<Vec<f32>>> {
        let mut embeddings = Vec::new();
        for text in texts {
            embeddings.push(self.embed(text).await?);
        }
        Ok(embeddings)
    }

    fn dimension(&self) -> usize {
        3
    }

    fn model_name(&self) -> &str {
        "mock-embedder"
    }
}

// Mock response generator
#[derive(Debug)]
struct MockGenerator;

#[async_trait::async_trait]
impl ResponseGenerator for MockGenerator {
    async fn generate_response(
        &self,
        query: &str,
        context_nodes: Vec<ScoredNode>,
        _options: &GenerationOptions,
    ) -> cheungfun_core::Result<cheungfun_core::types::GeneratedResponse> {
        let context_summary = if context_nodes.is_empty() {
            "No relevant context found.".to_string()
        } else {
            format!(
                "Based on {} relevant documents, here's what I found:",
                context_nodes.len()
            )
        };

        let response_content = format!(
            "{} Your question was: '{}'. This is a mock response that would normally be generated by an LLM using the retrieved context.",
            context_summary, query
        );

        Ok(cheungfun_core::types::GeneratedResponse {
            content: response_content,
            source_nodes: context_nodes.iter().map(|n| n.node.id).collect(),
            metadata: HashMap::new(),
            usage: None,
        })
    }

    async fn generate_response_stream(
        &self,
        query: &str,
        context_nodes: Vec<ScoredNode>,
        options: &GenerationOptions,
    ) -> cheungfun_core::Result<
        std::pin::Pin<Box<dyn futures::Stream<Item = cheungfun_core::Result<String>> + Send>>,
    > {
        let response = self
            .generate_response(query, context_nodes, options)
            .await?;
        let chunks: Vec<String> = response
            .content
            .split_whitespace()
            .map(|s| format!("{} ", s))
            .collect();

        Ok(Box::pin(futures::stream::iter(chunks.into_iter().map(Ok))))
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("üîç Query Pipeline Demo");
    println!("=======================\n");

    // Setup components
    let embedder = Arc::new(MockEmbedder);
    let generator = Arc::new(MockGenerator);

    // Test 1: Basic Memory Management
    println!("üß† Testing Conversation Memory");
    println!("{}", "-".repeat(40));

    let memory_config = ChatMemoryConfig {
        max_messages: Some(10),
        max_tokens: Some(2000),
        ..Default::default()
    };

    let mut memory = ChatMemoryBuffer::new(memory_config);

    // Simulate conversation
    let user_msg1 = ChatMessage {
        role: MessageRole::User,
        content: "What is machine learning?".to_string(),
        timestamp: chrono::Utc::now(),
        metadata: Some(HashMap::new()),
    };
    BaseMemory::add_message(&mut memory, user_msg1).await?;

    let ai_msg1 = ChatMessage {
        role: MessageRole::Assistant,
        content: "Machine learning is a subset of artificial intelligence...".to_string(),
        timestamp: chrono::Utc::now(),
        metadata: Some(HashMap::new()),
    };
    BaseMemory::add_message(&mut memory, ai_msg1).await?;

    let user_msg2 = ChatMessage {
        role: MessageRole::User,
        content: "Can you give me an example?".to_string(),
        timestamp: chrono::Utc::now(),
        metadata: Some(HashMap::new()),
    };
    BaseMemory::add_message(&mut memory, user_msg2).await?;

    let messages = BaseMemory::get_messages(&memory).await?;
    println!("Conversation history ({} messages):", messages.len());
    for (i, msg) in messages.iter().enumerate() {
        println!(
            "  {}. {:?}: {}",
            i + 1,
            msg.role,
            &msg.content[..msg.content.len().min(50)]
        );
    }

    let stats = memory.stats();
    println!(
        "Memory stats: {} messages, ~{} tokens",
        stats.message_count, stats.estimated_tokens
    );
    println!();

    // Test 2: Query Processing
    println!("‚ùì Testing Query Processing");
    println!("{}", "-".repeat(40));

    let queries = vec![
        "What is the purpose of this codebase?",
        "How does error handling work?",
        "Show me examples of async functions",
    ];

    for (i, query_text) in queries.iter().enumerate() {
        println!("Query {}: {}", i + 1, query_text);

        let query = Query {
            text: query_text.to_string(),
            embedding: Some(embedder.embed(query_text).await?),
            filters: HashMap::new(),
            top_k: 3,
            similarity_threshold: Some(0.5),
            search_mode: SearchMode::Vector,
        };

        // Simulate some retrieved context nodes
        let mock_nodes = create_mock_context_nodes();
        let scored_nodes: Vec<ScoredNode> = mock_nodes
            .into_iter()
            .enumerate()
            .map(|(idx, node)| ScoredNode {
                node,
                score: 0.8 - (idx as f32 * 0.1),
            })
            .collect();

        let generation_options = GenerationOptions {
            max_tokens: Some(200),
            temperature: Some(0.7),
            include_citations: true,
            ..Default::default()
        };

        let response = generator
            .generate_response(query_text, scored_nodes, &generation_options)
            .await?;

        println!(
            "  Response: {}...",
            &response.content[..response.content.len().min(100)]
        );
        println!("  Sources: {} documents", response.source_nodes.len());
        println!();
    }

    // Test 3: Streaming Response
    println!("üåä Testing Streaming Response");
    println!("{}", "-".repeat(40));

    let query_text = "Explain how streaming works";
    let mock_nodes = create_mock_context_nodes();
    let scored_nodes: Vec<ScoredNode> = mock_nodes
        .into_iter()
        .take(2)
        .enumerate()
        .map(|(idx, node)| ScoredNode {
            node,
            score: 0.9 - (idx as f32 * 0.1),
        })
        .collect();

    println!("Streaming response for: '{}'", query_text);
    print!("Response: ");

    use futures::StreamExt;
    let mut stream = generator
        .generate_response_stream(query_text, scored_nodes, &GenerationOptions::default())
        .await?;

    while let Some(chunk) = stream.next().await {
        match chunk {
            Ok(text) => print!("{}", text),
            Err(e) => println!("Error: {}", e),
        }
        tokio::time::sleep(std::time::Duration::from_millis(100)).await; // Simulate streaming delay
    }
    println!("\n");

    println!("‚úÖ Query pipeline demo completed!");

    Ok(())
}

fn create_mock_context_nodes() -> Vec<Node> {
    vec![
        Node {
            id: Uuid::new_v4(),
            content: "This function implements error handling using Result types and proper error propagation.".to_string(),
            metadata: {
                let mut map = HashMap::new();
                map.insert("file_path".to_string(), serde_json::Value::String("src/error.rs".to_string()));
                map.insert("function_name".to_string(), serde_json::Value::String("handle_error".to_string()));
                map
            },
            embedding: Some(vec![0.1, 0.2, 0.3]),
            sparse_embedding: None,
            relationships: cheungfun_core::relationships::NodeRelationships::default(),
            source_document_id: Uuid::new_v4(),
            chunk_info: cheungfun_core::types::ChunkInfo::new(Some(0), Some(100), 0),
            excluded_embed_metadata_keys: std::collections::HashSet::new(),
            excluded_llm_metadata_keys: std::collections::HashSet::new(),
            hash: Some("mock_hash_1".to_string()),
            text_template: "default_template".to_string(),
            metadata_template: "metadata_template".to_string(),
            metadata_separator: "\n".to_string(),
            mimetype: "text/plain".to_string(),
        },
        Node {
            id: Uuid::new_v4(),
            content: "Async functions in Rust use the async/await syntax for handling asynchronous operations efficiently.".to_string(),
            metadata: {
                let mut map = HashMap::new();
                map.insert("file_path".to_string(), serde_json::Value::String("src/async_utils.rs".to_string()));
                map.insert("function_name".to_string(), serde_json::Value::String("async_operation".to_string()));
                map
            },
            embedding: Some(vec![0.4, 0.5, 0.6]),
            sparse_embedding: None,
            relationships: cheungfun_core::relationships::NodeRelationships::default(),
            source_document_id: Uuid::new_v4(),
            chunk_info: cheungfun_core::types::ChunkInfo::new(Some(100), Some(200), 1),
            excluded_embed_metadata_keys: std::collections::HashSet::new(),
            excluded_llm_metadata_keys: std::collections::HashSet::new(),
            hash: Some("mock_hash_2".to_string()),
            text_template: "default_template".to_string(),
            metadata_template: "metadata_template".to_string(),
            metadata_separator: "\n".to_string(),
            mimetype: "text/plain".to_string(),
        },
    ]
}
