//! Response types for query results and LLM generation.
//!
//! This module defines the structures returned by query engines and
//! response generators, including metadata and usage information.

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use uuid::Uuid;

use super::ScoredNode;

/// Response from a query pipeline.
///
/// Contains the generated response along with the retrieved nodes
/// and metadata about the query execution.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryResponse {
    /// The generated response from the LLM.
    pub response: GeneratedResponse,

    /// The nodes that were retrieved and used for generation.
    pub retrieved_nodes: Vec<ScoredNode>,

    /// Metadata about the query execution.
    ///
    /// Common metadata includes:
    /// - `query_time`: Time taken to execute the query
    /// - `retrieval_time`: Time taken for retrieval
    /// - `generation_time`: Time taken for response generation
    /// - `total_time`: Total time for the entire pipeline
    /// - `retrieval_method`: Method used for retrieval
    /// - `model_used`: LLM model used for generation
    pub query_metadata: HashMap<String, serde_json::Value>,
}

/// Response generated by an LLM.
///
/// Contains the generated text content along with metadata about
/// the generation process and token usage.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GeneratedResponse {
    /// The generated text content.
    pub content: String,

    /// IDs of source nodes used for generation.
    ///
    /// These correspond to the nodes that were provided as context
    /// to the LLM for generating the response.
    pub source_nodes: Vec<Uuid>,

    /// Generation metadata.
    ///
    /// Common metadata includes:
    /// - `model`: Model name used for generation
    /// - `temperature`: Temperature setting used
    /// - `max_tokens`: Maximum tokens setting
    /// - `finish_reason`: Why generation stopped (length, stop, etc.)
    /// - `created_at`: Timestamp when response was generated
    /// - `provider`: LLM provider used
    pub metadata: HashMap<String, serde_json::Value>,

    /// Token usage information (if available).
    pub usage: Option<TokenUsage>,
}

/// Token usage information for LLM calls.
///
/// Provides detailed information about token consumption,
/// which is useful for cost tracking and optimization.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct TokenUsage {
    /// Number of tokens in the prompt/input.
    pub prompt_tokens: usize,

    /// Number of tokens in the completion/output.
    pub completion_tokens: usize,

    /// Total tokens used (prompt + completion).
    pub total_tokens: usize,
}

/// Format options for response generation.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum ResponseFormat {
    /// Plain text response.
    Text,

    /// Markdown formatted response.
    Markdown,

    /// JSON structured response.
    Json,

    /// Custom format with specific instructions.
    Custom {
        /// Format description or template.
        format: String,
    },
}

/// Options for controlling response generation.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenerationOptions {
    /// Maximum tokens to generate.
    pub max_tokens: Option<usize>,

    /// Temperature for generation (0.0 to 2.0).
    ///
    /// Higher values make output more random, lower values more deterministic.
    pub temperature: Option<f32>,

    /// System prompt override.
    ///
    /// If provided, this will override the default system prompt.
    pub system_prompt: Option<String>,

    /// Whether to include source citations in the response.
    pub include_citations: bool,

    /// Response format.
    pub format: ResponseFormat,

    /// Additional generation parameters.
    ///
    /// Provider-specific parameters can be passed through this field.
    pub additional_params: HashMap<String, serde_json::Value>,
}

/// Context for retrieval operations.
///
/// Provides additional context that can influence retrieval behavior,
/// such as conversation history and user preferences.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct RetrievalContext {
    /// Previous conversation history.
    pub chat_history: Vec<ChatMessage>,

    /// User context and preferences.
    ///
    /// Can include user ID, preferences, access permissions, etc.
    pub user_context: HashMap<String, serde_json::Value>,

    /// Session identifier for tracking conversations.
    pub session_id: Option<String>,

    /// Additional context metadata.
    pub metadata: HashMap<String, serde_json::Value>,
}

/// A message in a conversation.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// Role of the message sender.
    pub role: MessageRole,

    /// Content of the message.
    pub content: String,

    /// Timestamp when the message was created.
    pub timestamp: DateTime<Utc>,

    /// Optional message metadata.
    pub metadata: Option<HashMap<String, serde_json::Value>>,
}

/// Role of a message sender in a conversation.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum MessageRole {
    /// Message from the user.
    User,

    /// Message from the AI assistant.
    Assistant,

    /// System message (instructions, context, etc.).
    System,

    /// Message from a tool or function call.
    Tool,
}

impl QueryResponse {
    /// Create a new query response.
    pub fn new(response: GeneratedResponse, retrieved_nodes: Vec<ScoredNode>) -> Self {
        Self {
            response,
            retrieved_nodes,
            query_metadata: HashMap::new(),
        }
    }

    /// Add metadata to the query response.
    pub fn with_metadata<K, V>(mut self, key: K, value: V) -> Self
    where
        K: Into<String>,
        V: Into<serde_json::Value>,
    {
        self.query_metadata.insert(key.into(), value.into());
        self
    }

    /// Get the response content.
    pub fn content(&self) -> &str {
        &self.response.content
    }

    /// Get the number of retrieved nodes.
    pub fn num_retrieved(&self) -> usize {
        self.retrieved_nodes.len()
    }
}

impl GeneratedResponse {
    /// Create a new generated response.
    pub fn new<S: Into<String>>(content: S) -> Self {
        Self {
            content: content.into(),
            source_nodes: Vec::new(),
            metadata: HashMap::new(),
            usage: None,
        }
    }

    /// Add a source node ID.
    pub fn with_source_node(mut self, node_id: Uuid) -> Self {
        self.source_nodes.push(node_id);
        self
    }

    /// Set token usage information.
    pub fn with_usage(mut self, usage: TokenUsage) -> Self {
        self.usage = Some(usage);
        self
    }

    /// Add metadata.
    pub fn with_metadata<K, V>(mut self, key: K, value: V) -> Self
    where
        K: Into<String>,
        V: Into<serde_json::Value>,
    {
        self.metadata.insert(key.into(), value.into());
        self
    }

    /// Check if the response has usage information.
    pub fn has_usage(&self) -> bool {
        self.usage.is_some()
    }

    /// Get the total tokens used, if available.
    pub fn total_tokens(&self) -> Option<usize> {
        self.usage.as_ref().map(|u| u.total_tokens)
    }
}

impl TokenUsage {
    /// Create new token usage information.
    pub fn new(prompt_tokens: usize, completion_tokens: usize) -> Self {
        Self {
            prompt_tokens,
            completion_tokens,
            total_tokens: prompt_tokens + completion_tokens,
        }
    }

    /// Calculate cost based on token pricing.
    ///
    /// # Arguments
    ///
    /// * `prompt_price_per_1k` - Price per 1000 prompt tokens
    /// * `completion_price_per_1k` - Price per 1000 completion tokens
    pub fn calculate_cost(&self, prompt_price_per_1k: f64, completion_price_per_1k: f64) -> f64 {
        let prompt_cost = (self.prompt_tokens as f64 / 1000.0) * prompt_price_per_1k;
        let completion_cost = (self.completion_tokens as f64 / 1000.0) * completion_price_per_1k;
        prompt_cost + completion_cost
    }
}

impl Default for GenerationOptions {
    fn default() -> Self {
        Self {
            max_tokens: None,
            temperature: None,
            system_prompt: None,
            include_citations: false,
            format: ResponseFormat::Text,
            additional_params: HashMap::new(),
        }
    }
}

impl ChatMessage {
    /// Create a new chat message.
    pub fn new<S: Into<String>>(role: MessageRole, content: S) -> Self {
        Self {
            role,
            content: content.into(),
            timestamp: Utc::now(),
            metadata: None,
        }
    }

    /// Create a user message.
    pub fn user<S: Into<String>>(content: S) -> Self {
        Self::new(MessageRole::User, content)
    }

    /// Create an assistant message.
    pub fn assistant<S: Into<String>>(content: S) -> Self {
        Self::new(MessageRole::Assistant, content)
    }

    /// Create a system message.
    pub fn system<S: Into<String>>(content: S) -> Self {
        Self::new(MessageRole::System, content)
    }

    /// Add metadata to the message.
    pub fn with_metadata(mut self, metadata: HashMap<String, serde_json::Value>) -> Self {
        self.metadata = Some(metadata);
        self
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_token_usage() {
        let usage = TokenUsage::new(100, 50);
        assert_eq!(usage.prompt_tokens, 100);
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.total_tokens, 150);

        // Test cost calculation (example: $0.001 per 1k prompt, $0.002 per 1k completion)
        let cost = usage.calculate_cost(0.001, 0.002);
        assert_eq!(cost, 0.0001 + 0.0001); // (100/1000)*0.001 + (50/1000)*0.002
    }

    #[test]
    fn test_generated_response() {
        let response = GeneratedResponse::new("Test response")
            .with_source_node(Uuid::new_v4())
            .with_usage(TokenUsage::new(10, 5))
            .with_metadata("model", "gpt-4");

        assert_eq!(response.content, "Test response");
        assert_eq!(response.source_nodes.len(), 1);
        assert!(response.has_usage());
        assert_eq!(response.total_tokens(), Some(15));
    }

    #[test]
    fn test_chat_message() {
        let msg = ChatMessage::user("Hello, world!");
        assert_eq!(msg.role, MessageRole::User);
        assert_eq!(msg.content, "Hello, world!");
        assert!(msg.metadata.is_none());
    }
}
